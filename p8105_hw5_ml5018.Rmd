---
title: "p8105_hw5_ml5018"
author: "Luan Mengxiao"
date: 2023-11-02
output: github_document
---

```{r setup, message = FALSE}
library(tidyverse)

options(tibble.print_min = 5)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## summary of data

Describe the raw data.

```{r}
homicides_data = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names()
```

This data set contains data on homicides in 50 large U.S. cities, gathered by the Washington Post. The Post has mapped more than 52,000 homicides in major American cities over the past decade and found that across the country, there are areas where murder is common but arrests are rare.

The data set consists of `r nrow(homicides_data)` observations of `r ncol(homicides_data)` variables. A more detailed summary of the raw data is posted below.

```{r}
skimr::skim(homicides_data)
```

## data manipulation

Create a `city_state` variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
homicides_data = 
  homicides_data |>
  mutate(city_state = str_c(city, ", ", state))

homicides_number = 
  homicides_data |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = 
      sum(disposition %in% c("Closed without arrest","Open/No arrest")))
homicides_number
```

## proportion test

For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
Baltimore_number = 
  homicides_number |>
  filter(city_state == "Baltimore, MD")

Baltimore_result = 
  prop.test(x = pull(Baltimore_number, var = 3), 
          n = pull(Baltimore_number, var = 2)) |>
  broom::tidy()
Baltimore_result

list(
  estimate = pull(Baltimore_result, estimate),
  CI_lower = pull(Baltimore_result, conf.low),
  CI_upper = pull(Baltimore_result, conf.high)
  ) |>
  bind_rows() |>
  knitr::kable()

save(Baltimore_result, file = "result/baltimore_result.RData")
```

Now run `prop.test` for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of `purrr::map`, `purrr::map2`, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
unsolved_prop = function(city_state_name) {
  city_number = 
    homicides_number |>
    filter(city_state == city_state_name)
  
  city_result = 
    prop.test(x = pull(city_number, var = 3), n = pull(city_number, var = 2)) |>
    broom::tidy() |>
    select(estimate, conf.low, conf.high)
  
  city_result
}

city_name = unique(pull(homicides_data, city_state))
result = 
  expand_grid(city_name) |>
  mutate(test_result = map(city_name, unsolved_prop)) |>
  unnest(test_result)
result
```

Create a plot that shows the estimates and CIs for each city – check out `geom_errorbar` for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
result |>
  mutate(city_name = reorder(city_name, estimate)) |>
  ggplot(aes(x = city_name, y = estimate)) +
  geom_point(aes(y = estimate)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 45, size = 5)) +
  labs(x = "city_state", 
       y = "estimate and 95% CI", 
       title = "Proportion Test Results for Unsolved Homisides")
```

# Problem 2

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

* Start with a dataframe containing all file names; the `list.files` function will help
* Iterate over file names and read in data for each subject using `purrr::map` and saving the result as a new variable in the dataframe
* Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
* Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
file_name = 
  list.files(path = "./data/longitudinal_data/", pattern = ".csv")

longitudinal_data = 
  data_frame(file = file_name) |>
  mutate(
    file_path = str_c("data/longitudinal_data/", file),
    data = map(file_path, read_csv)) |>
  unnest(data) |>
  janitor::clean_names() |>
  separate(file, into = c("arm", "subject_id"), sep = "_") |>
  mutate(
    arm = as.factor(case_match(arm, "con" ~ "control", "exp" ~ "experiment")),
    subject_id = str_replace(subject_id, ".csv", "")
  ) |>
  pivot_longer(week_1:week_8, 
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation") |>
  select(arm, subject_id, week, observation)
longitudinal_data

longitudinal_data |>
  ggplot(aes(x = week, y = observation, 
             group = subject_id, color = subject_id)) +
  geom_line() +
  facet_grid(.~arm) +
  labs(x = "week",
       y = "observation",
       title = "Observation over time for Each Subject")
```

It can be seen from the spaghetti plot that there exists an obvious difference between the observations of two groups, with an increasing trend and consequenct higher value in the experiment arm compared with the control arm. The observation of subjects all subjects in experiment group reaches a positive value, while that of control group fluctuates around 0 in 8 weeks.

# Problem 3

## mu = 0

First set the following design elements:

* Fix n=30
* Fix σ=5

Set μ=0. Generate 5000 datasets from the model: x∼Normal[μ,σ]

For each dataset, save μ̂ and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of `t.test`.

```{r}
sim_test = function(mu, n_obs = 30, sigma = 5){
  sim_sample = rnorm(n = n_obs, mean = mu, sd = sigma)
  
  sim_result = 
    t.test(sim_sample) |>
    broom::tidy() |>
    select(estimate, p.value)
  
  sim_result
}

result_mu_0 = 
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |>
  mutate(result = map(mu, sim_test)) |>
  unnest(result)

result_mu_0
```

## mu in (0,1,2,3,4,5,6)

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

* Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
* Make a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r}
result_mu = 
  expand.grid(
    mu = c(1,2,3,4,5,6),
    iter = 1:5000
  ) |>
  mutate(result = map(mu, sim_test)) |>
  unnest(result) |>
  bind_rows(result_mu_0) |>
  select(-iter)

result_mu |>
  group_by(mu) |>
  summarize(total = n(),
            rejected = sum(p.value < 0.05)) |>
  mutate(proportion = rejected / total) |>
  ggplot(aes(x = mu, y = proportion)) +
  scale_x_continuous(limits = c(0,6), breaks = seq(0,6,1)) + 
  geom_point() +
  geom_line() +
  theme() +
  labs(x = "mu",
       y = "proportion of rejection",
       title = "Rejected Proportion in groups with different mu",
       caption = "Association bewteen Effect Size and Power")

result_mu |>
  filter(p.value < 0.05) |>
  group_by(mu) |>
  summarize(n_reject = n()) |>
  mutate(power = n_reject / 5000)
```

It can be concluded from the plot above that the proportion of rejection tends to increase as the true mean increases, i.e. the power of the test increases as the effect size increases.

```{r}
result_mu |>
  group_by(mu) |>
  summarize(mu_hat = mean(estimate)) |>
  ggplot(aes(x = mu, y = mu_hat)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(limits = c(0,6), breaks = seq(0,6,1)) +
  labs(x = "true mean",
       y = "average of estimated mean",
       title = "Estimated Mean versus True Mean")

rejected_data = 
  result_mu |>
  filter(p.value < 0.05) |>
  group_by(mu) |>
  summarize(mu_hat = mean(estimate))
total_data = 
  result_mu |>
  group_by(mu) |>
  summarize(mu_hat = mean(estimate))
ggplot(data = total_data, aes(x = mu, y = mu_hat)) +
  geom_line(data = total_data, aes(color = "blue")) +
  geom_line(data = rejected_data, aes(color = "red")) +
  geom_point(data = total_data, aes(colour = "blue")) +
  geom_point(data = rejected_data, aes(colour = "red")) +
  scale_x_continuous(limits = c(0,6), breaks = seq(0,6,1)) +
  scale_color_manual(" ", values = c("blue" = "blue", "red" = "red"),
                     labels = c("all estimates","rejected estimates")) +
  labs(x = "true mean",
       y = "average of estimated mean",
       title = "All versus Rejected Estimates")
```

It can be seen that the sample average of mu across tests for which the null is rejected is not quite equal to the true value. This is because, when the null hypothesis is rejected we have more confidence that the sample mean we gain is not as close to the true mean as when the null hypothesis holds. In other words, there is larger discrepancy between the true value and the estimate.